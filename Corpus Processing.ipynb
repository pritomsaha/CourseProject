{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Corpus Processing.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOqWO3HKpX+pdKNBCFTl1Jj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-7KjWcpQNDl","executionInfo":{"status":"ok","timestamp":1607847136044,"user_tz":-360,"elapsed":31919,"user":{"displayName":"Pritom Saha Akash","photoUrl":"","userId":"10957475970477080821"}},"outputId":"6babcb84-18cb-4d47-e1ce-13f52a62419b"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LYFPHmhdRnxE"},"source":["import os\n","os.chdir('/content/drive/My Drive/Project-CS410')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zv0QwQsSXCLU"},"source":["# Get Corpus"]},{"cell_type":"code","metadata":{"id":"_8-SYB-0WyJA"},"source":["from bs4 import BeautifulSoup as bs\r\n","import numpy as np\r\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeSGx5puXs6S"},"source":["# regular expression for checking whether \"gore\" and/or \"bush\" exist in a text. \r\n","RE = re.compile(r'([^a-z]|^)gore([^a-z]|$)|([^a-z]|^)bush([^a-z]|$)')\r\n","keyword_filter = False\r\n","\r\n","# May 2000 to Ocbtober 2000\r\n","dirs = ['2000/05','2000/06','2000/07','2000/08','2000/09','2000/10']\r\n","\r\n","# July 2000 to December 2001\r\n","# dirs = ['2000/07','2000/08','2000/09','2000/10','2000/11','2000/12','2001/01','2001/02','2001/03','2001/04','2001/05','2001/06','2001/07','2001/08','2001/09','2001/10','2001/11','2001/12']\r\n","\r\n","storage_name = 'corpus_may2000-oct2000.txt'\r\n","# storage_name = 'corpus_july2000-dec2001.txt'\r\n","\r\n","fout = open(storage_name, 'w', encoding='utf-8')\r\n","for dir_ in dirs:\r\n","    year, month = dir_.split('/') \r\n","    days = os.listdir(year+'/'+month)\r\n","    for day in days:\r\n","        xml_files = os.listdir(year+'/'+month+'/'+day)\r\n","        for xml_file in xml_files:\r\n","            doc = \"\"\r\n","            # Read the XML file\r\n","            with open(year+'/'+month+'/'+day+'/'+xml_file, \"r\", encoding='utf-8') as file:\r\n","                # Read each line in the file, readlines() returns a list of lines\r\n","                content = file.readlines()\r\n","                # Combine the lines in the list into a string\r\n","                content = \"\".join(content)\r\n","                bs_content = bs(content, \"html\")\r\n","                if keyword_filter:\r\n","                    # get those paragraphs that has the keyword in its text.\r\n","                    paragraphs = [par.getText().lower().strip() for par in bs_content.find_all('p') if par and RE.search(par.getText().lower())]\r\n","                else:\r\n","                    paragraphs = [par.getText().lower().strip() for par in bs_content.find_all('p')]\r\n","                doc = \" \"+ \" \".join(paragraphs)\r\n","                doc = doc.replace(\"\\n\", \"\")\r\n","                doc = doc.replace(\"\\t\", \" \").strip()\r\n","                if doc == \"\" or not doc:\r\n","                    continue\r\n","                line = year+'-'+month+'-'+day + \"\\t\" + doc + \"\\n\"\r\n","                fout.write(line)\r\n","        print(year+'-'+month+'-'+day)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSDUAs2bY2g8"},"source":["# Preprocess Corpus"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iTJwAyw6Rl3P","executionInfo":{"status":"ok","timestamp":1607847168222,"user_tz":-360,"elapsed":3310,"user":{"displayName":"Pritom Saha Akash","photoUrl":"","userId":"10957475970477080821"}},"outputId":"22aefc93-edbd-4f7f-d624-4f8aaa219172"},"source":["import nltk\n","nltk.download('brown')\n","nltk.download('names')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package names to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/names.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"eqcNsJ14QV-0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607847185331,"user_tz":-360,"elapsed":15917,"user":{"displayName":"Pritom Saha Akash","photoUrl":"","userId":"10957475970477080821"}},"outputId":"22676835-73f5-4152-ce96-01aa83450a07"},"source":["!pip install normalise\n","import numpy as np\n","import pandas as pd\n","import multiprocessing as mp\n","import string\n","import spacy \n","import en_core_web_sm\n","from nltk.tokenize import word_tokenize\n","from sklearn.base import TransformerMixin, BaseEstimator\n","from normalise import normalise\n","\n","nlp = en_core_web_sm.load()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting normalise\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/2d/f06cf3d3714502dec10e19238a5da201b71ce198165beda9c1adaf5063da/normalise-0.1.8-py3-none-any.whl (15.7MB)\n","\u001b[K     |████████████████████████████████| 15.7MB 189kB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from normalise) (0.22.2.post1)\n","Collecting roman\n","  Downloading https://files.pythonhosted.org/packages/c3/9e/47df0bf47ccd7e9bbbf0a539ac86e45ded37c34dba544a0a2e5d01ce5f88/roman-3.3-py2.py3-none-any.whl\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.4.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from normalise) (1.18.5)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from normalise) (3.2.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->normalise) (0.17.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->normalise) (1.15.0)\n","Installing collected packages: roman, normalise\n","Successfully installed normalise-0.1.8 roman-3.3\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.semi_supervised.label_propagation module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.semi_supervised. Anything that cannot be imported from sklearn.semi_supervised is now part of the private API.\n","  warnings.warn(message, FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LabelPropagation from version 0.18 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n","  UserWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"P7UNz4uTRapb"},"source":["class TextPreprocessor(BaseEstimator, TransformerMixin):\n","    def __init__(self,\n","                 variety=\"BrE\",\n","                 user_abbrevs={},\n","                 n_jobs=1):\n","        \"\"\"\n","        Text preprocessing transformer includes steps:\n","            1. Text normalization\n","            2. Punctuation removal\n","            3. Stop words removal\n","            4. Lemmatization\n","        \n","        variety - format of date (AmE - american type, BrE - british format) \n","        user_abbrevs - dict of user abbreviations mappings (from normalise package)\n","        n_jobs - parallel jobs to run\n","        \"\"\"\n","        self.variety = variety\n","        self.user_abbrevs = user_abbrevs\n","        self.n_jobs = n_jobs\n","\n","    def fit(self, X, y=None):\n","        return self\n","\n","    def transform(self, X, *_):\n","        X_copy = X.copy()\n","\n","        partitions = 1\n","        cores = mp.cpu_count()\n","        if self.n_jobs <= -1:\n","            partitions = cores\n","        elif self.n_jobs <= 0:\n","            return X_copy.apply(self._preprocess_text)\n","        else:\n","            partitions = min(self.n_jobs, cores)\n","\n","        data_split = np.array_split(X_copy, partitions)\n","        pool = mp.Pool(cores)\n","        data = pd.concat(pool.map(self._preprocess_part, data_split))\n","        pool.close()\n","        pool.join()\n","\n","        return data\n","\n","    def _preprocess_part(self, part):\n","        return part.apply(self._preprocess_text)\n","\n","    def _preprocess_text(self, text):\n","        normalized_text = self._normalize(text)\n","        doc = nlp(normalized_text)\n","        removed_punct = self._remove_punct(doc)\n","        removed_stop_words = self._remove_stop_words(removed_punct)\n","        return self._lemmatize(removed_stop_words)\n","\n","    def _normalize(self, text):\n","        # some issues in normalise package\n","        try:\n","            return ' '.join(normalise(text, variety=self.variety, user_abbrevs=self.user_abbrevs, verbose=False))\n","        except:\n","            return text\n","\n","    def _remove_punct(self, doc):\n","        return [t for t in doc if t.text not in string.punctuation]\n","\n","    def _remove_stop_words(self, doc):\n","        return [t for t in doc if not t.is_stop]\n","\n","    def _lemmatize(self, doc):\n","        return ' '.join([t.lemma_ for t in doc])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YE0R-vuER3Q3"},"source":["df = pd.read_csv('datasets/corpus_july2000-dec2001.txt', sep='\\t', header=None)\n","df[1] = TextPreprocessor(n_jobs=-1).transform(df[1])\n","df.to_csv('datasets/corpus_july2000-dec2001_cleaned.txt', sep=\"\\t\", header=False, index=False)"],"execution_count":null,"outputs":[]}]}