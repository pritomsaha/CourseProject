{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ITMTF.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aIxoDHr8VYsC","executionInfo":{"status":"ok","timestamp":1607879517650,"user_tz":-360,"elapsed":109097,"user":{"displayName":"Trisha Das","photoUrl":"","userId":"15362276246443347108"}},"outputId":"7e34af58-31d7-4a87-8bc6-a805da9a23ad"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iRsbvPlZZL6Y"},"source":["import os\n","os.chdir('/content/drive/My Drive/Project-CS410')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPfUv2wOPMzf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607879533136,"user_tz":-360,"elapsed":7172,"user":{"displayName":"Trisha Das","photoUrl":"","userId":"15362276246443347108"}},"outputId":"f7102567-d14f-462a-8780-951e889c4bc7"},"source":["import numpy as np\n","import pandas as pd\n","from numba_plsa.corpus import CorpusBuilder\n","from numba_plsa.plsa import PLSAModel\n","stopword_file = 'datasets/stop.txt'\n","import scipy\n","from statsmodels.tsa.stattools import grangercausalitytests\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ZT4RUaZs9HyS"},"source":["# Topic Modeling"]},{"cell_type":"code","metadata":{"id":"nTli-jtxVjfL"},"source":["def get_stopwords(fname):\n","  with open(fname, 'rb') as f:\n","    return set(\n","      line.split(b' ', 1)[0].strip() for line in f if line[0] not in [' ', '|']\n","    )\n","def print_title(txt):\n","  print (\"\\n{0}\\n{1}\".format(txt, '=' * len(txt)))\n","\n","def get_topic_coverage(time_entries, topic_doc_matrix, time_unit=1):\n","  timestamps = sorted(set(time_entries))\n","  topic_coverage = np.zeros((len(timestamps), topic_doc_matrix.shape[1]))\n","  for i, timestamp in enumerate(timestamps):\n","    idx = time_entries==timestamp\n","    coverage = np.sum(topic_doc_matrix[idx,:], axis=0)\n","    topic_coverage[i] = coverage\n","  topic_coverage = pd.DataFrame(data=topic_coverage)\n","  return topic_coverage\n","\n","def build_corpus(docs, silent=False):\n","  stopwords = get_stopwords(stopword_file)\n","  if silent==False:\n","    print_title(\"Building corpus\")\n","  CB = CorpusBuilder(stopwords=stopwords, min_len=3, max_len=8)\n","  doc_count = 0\n","  for i, doc in enumerate(docs):\n","    CB.add_document(name=str(i), text=doc)\n","  return CB  \n","\n","def run_plsa(CB, term_topic_prior, mu=0, n_topics=10, n_iter=100, min_count=5, method='numba', silent=False):\n","  doc_term = CB.get_doc_term()\n","  if silent==False:\n","    print_title(\"\\nRunning pLSA\")\n","  model = PLSAModel()\n","  model.train(doc_term, n_topics=n_topics, n_iter=n_iter, min_count=min_count, method=method, term_topic_prior=term_topic_prior, mu=mu)\n","\n","  if silent==False:\n","    print_title(\"\\nTop topic terms\")\n","    top_topic_words = model.top_topic_terms(5, normalized=True)\n","    for i in range(n_topics):\n","      print (\"Topic {0}: {1}\".format(\n","        i + 1, ', '.join(CB.get_term(j) for j in top_topic_words[i])\n","      ))\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LpihaykTX6GF"},"source":["# Topic-level Causality Modeling"]},{"cell_type":"code","metadata":{"id":"uPZ2X73CYKwh"},"source":["def smoothing(series, window=3):\n","    series1 = series.rolling(window).mean()\n","    series1=series1-series1.shift(1)\n","    series1=series1.bfill()\n","    return series1\n","\n","def granger_test(time_series, topic_coverage, lag=5, alpha=0.05):\n","  # smooth both time_series and topic coverage data\n","  smoothed_time_series = smoothing(time_series)\n","  smoothed_topic_coverage = topic_coverage.copy()\n","  for i in range(topic_coverage.shape[1]):\n","    smoothed_topic_coverage[:][i] = smoothing(smoothed_topic_coverage[:][i])\n","\n","  causal_topics=[]\n","  causal_lags=[]\n","  #per topic granger test\n","  for i in range(topic_coverage.shape[1]):\n","    df_granger = pd.DataFrame(columns = ['time', 'topic'], data=zip(smoothed_time_series, smoothed_topic_coverage[:][i]))\n","    gc_res= grangercausalitytests(df_granger, lag, verbose=False)\n","    for j in range(1,lag+1):\n","      if(gc_res.get(j)[0].get('ssr_ftest')[1]<alpha):\n","        if(i not in causal_topics):\n","          causal_topics.append(i)\n","          causal_lags.append(j)\n","  return causal_topics, causal_lags"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Axdm_V6YZGN"},"source":["#Word-level Causality Modeling"]},{"cell_type":"code","metadata":{"id":"Or0OSjm4Y0SX"},"source":["#for each term (each column in doc_term_matrix_sum), perform Pearson correlation and return r and significance value\n","def get_word_level_pearson(word_freq, time_series):\n","  pearson_matrix=np.zeros(doc_term_matrix.shape[1])\n","  pearson_sig_matrix=np.zeros(doc_term_matrix.shape[1])\n","  for i in range(doc_term_matrix.shape[1]):\n","    r,sig=scipy.stats.pearsonr(doc_term_date_sum_matrix[i], time_series)\n","    pearson_matrix[i]=r\n","    pearson_sig_matrix[i]=sig\n","  return pearson_matrix, pearson_sig_matrix\n","\n","#function that converts a list of significant values to a corresponding list of probabilities, based on the formula at the bottom of page 3\n","def get_prob_from_sig(sig, cutoff):\n","  a=np.array(sig)\n","  weights=a-cutoff\n","  prob=weights/weights.sum()\n","  return list(prob)\n","\n","def generate_topic_priors(plsa_model, num_top_terms, num_topics, silent=False):\n","  ''' input: plsa_model: plsa object from topic modelling\n","            num_top_terms: number of top terms being retrieved from plsa output\n","            num_topics: number of topics used in plsa\n","      output: a (num_topics x num_terms) matrix reflecting unified topic priors from all groups of significant terms\n","  '''\n","  \n","  #get r and significance values for top terms in each topic\n","  top_topic_terms_indices=plsa_model.top_topic_terms(num_top_terms, normalized=True)\n","  top_topic_terms_r=np.zeros(top_topic_terms_indices.shape)\n","  top_topic_terms_sig=np.zeros(top_topic_terms_indices.shape)\n","  for i in range(num_topics):\n","    for j in range(num_top_terms):\n","      top_topic_terms_r[i,j]=word_level_pearson_matrix[top_topic_terms_indices[i,j]]\n","      top_topic_terms_sig[i,j]=word_level_pearson_sig_matrix[top_topic_terms_indices[i,j]]\n","\n","  np.set_printoptions(suppress=True)\n","  if silent==False:\n","    print('Top topic terms:\\n',top_topic_terms_indices)\n","    print('\\nCorrelation r of top topic terms:\\n',top_topic_terms_r)\n","    print('\\nCorrelation significance value of top topic terms:\\n', top_topic_terms_sig)\n","\n","  #initializing list of topic models for prior, each list item is a numpy array of shape (num_topics, num_terms)\n","  topic_prior_list=[]\n","  num_terms=doc_term_matrix.shape[1]\n","  significant_terms_df=pd.DataFrame(columns=['topic', 'term_index', 'sig', 'r', 'r_sign']) #initialize a df to store significant terms summary\n","\n","  for i in range(num_topics): #loop through each topic\n","    if silent == False:\n","      print('\\nTopic '+str(i)+':')\n","    \n","    pos_indices=[]\n","    pos_sig=[]\n","    neg_indices=[]\n","    neg_sig=[]\n","\n","    for j in range(num_top_terms): #loop through each term\n","      if top_topic_terms_sig[i, j]<.05: #if term is significant\n","        #append term index and significance value to a list based on the correlation sign\n","        if top_topic_terms_r[i, j]>0: \n","          pos_indices.append(top_topic_terms_indices[i,j])\n","          pos_sig.append(1-top_topic_terms_sig[i, j])\n","        else:\n","          neg_indices.append(top_topic_terms_indices[i,j])\n","          neg_sig.append(1-top_topic_terms_sig[i, j])\n","        \n","        #add row to df for summary\n","        new_row={'topic': i, 'term_index': top_topic_terms_indices[i, j], 'sig': 1-top_topic_terms_sig[i, j], 'r': top_topic_terms_r[i, j], 'r_sign': '+' if top_topic_terms_r[i, j]>0 else '-'}\n","        significant_terms_df=significant_terms_df.append(new_row, ignore_index=True)\n","    \n","    #process each group of significant terms if group size is at least 10% of the other group size\n","    sig_cutoff=.95\n","\n","    if len(pos_indices)/max(len(neg_indices), 1) >= .1:\n","      pos_prob=get_prob_from_sig(pos_sig, sig_cutoff)\n","      if silent==False:\n","        print('pos indices', pos_indices)\n","        print('pos sig', pos_sig)\n","        print('pos prob', pos_prob)\n","      topic_model=np.zeros((num_topics, num_terms))\n","      topic_model[i, pos_indices]=pos_prob\n","      topic_prior_list.append(topic_model)\n","      if silent==False:\n","        print('=> Added topic model for positive terms')\n","\n","    if len(neg_indices)/max(len(neg_indices), 1) >= .1:\n","      neg_prob=get_prob_from_sig(neg_sig, sig_cutoff)\n","      if silent==False:\n","        print('neg indices', neg_indices)\n","        print('neg sig', neg_sig)  \n","        print('neg prob', neg_prob)\n","      topic_model=np.zeros((num_topics, num_terms))\n","      topic_model[i, neg_indices]=neg_prob\n","      topic_prior_list.append(topic_model)\n","      if silent==False:\n","        print('=> Added topic model for negative terms')\n","\n","    if silent==False: \n","      print('\\nSignificant terms summary: \\n', significant_terms_df)\n","    return sum(topic_prior_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WRwOmRwbrkRk"},"source":["# Prepare data"]},{"cell_type":"markdown","metadata":{"id":"3fNAeBk41Whf"},"source":["Get Text Data"]},{"cell_type":"code","metadata":{"id":"mBvYLTfR1FyH"},"source":["data_path = 'datasets/corpus_may2000-oct2000_cleaned.txt'\r\n","text_data = pd.read_csv(data_path, sep='\\t', header=None).values\r\n","time_entries, docs = text_data[:,0], text_data[:,1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pnMmvy811Zzp"},"source":["Get Time Series Data"]},{"cell_type":"code","metadata":{"id":"5_exRmqOrorZ"},"source":["#read time series data for Democratics\r\n","time_series_matrix_Dem= pd.read_csv(\"datasets/May_Oct_Dem.csv\",parse_dates=['Date'], index_col='Date')\r\n","#read time series data for Republicans\r\n","time_series_matrix_Rep=pd.read_csv(\"datasets/May_Oct_Rep.csv\",parse_dates=['Date'], index_col='Date')\r\n","\r\n","#time series democrates \r\n","time_series_Dem=time_series_matrix_Dem['AvgPrice']/(time_series_matrix_Rep['AvgPrice']+time_series_matrix_Dem['AvgPrice'])\r\n","time_series_Dem=time_series_Dem.bfill()\r\n","\r\n","#time series republicans \r\n","time_series_Rep=time_series_matrix_Rep['AvgPrice']/(time_series_matrix_Rep['AvgPrice']+time_series_matrix_Dem['AvgPrice'])\r\n","time_series_Rep=time_series_Rep.bfill()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QlAZlpCo1sta"},"source":["# Run ITMTF"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LepgD7uBWJyc","executionInfo":{"status":"ok","timestamp":1607881990163,"user_tz":-360,"elapsed":76963,"user":{"displayName":"Pritom Saha Akash","photoUrl":"","userId":"10957475970477080821"}},"outputId":"946c7b3f-4485-4e13-f098-ec9e5b077409"},"source":["CB = build_corpus(docs, silent=True)\n","n_topics = 10\n","doc_term_matrix = CB.get_doc_term()\n","time_series_data = time_series_Dem\n","\n","term_topic_prior = np.zeros((n_topics, doc_term_matrix.shape[1]))\n","causal_topics = []\n","iter = 5\n","for it in range(iter):\n","  print(\"\\n\\n=> Iternation: \", it)\n","  mu_vec = np.zeros(n_topics)\n","  mu_vec[:] = 100\n","  plsa_model = run_plsa(CB, n_topics=n_topics, term_topic_prior=term_topic_prior, mu=mu_vec, n_iter=200, silent=False)\n","  topic_coverage = get_topic_coverage(time_entries, plsa_model.topic_doc)\n","  causal_topics, causal_lags = granger_test(time_series_data, topic_coverage)\n","  print(\"Causal Topics:, \", np.array(causal_topics)+1)\n","  doc_term_date_sum_matrix=pd.DataFrame(doc_term_matrix, index=time_entries).groupby(level=0).sum()\n","  word_level_pearson_matrix, word_level_pearson_sig_matrix=get_word_level_pearson(doc_term_date_sum_matrix, time_series_data)\n","  \n","  num_top_terms=100\n","  term_topic_prior = generate_topic_priors(plsa_model, num_top_terms, n_topics, silent=True)"],"execution_count":46,"outputs":[{"output_type":"stream","text":["\n","\n","=> Iternation:  0\n","\n","\n","Running pLSA\n","=============\n","\n","        Running numba pLSA algorithm\n","        ============================\n","        Number of iterations: 200\n","        Number of documents: 3625 / 3625 before min_count (5)\n","        Number of terms: 6301 / 14916 before min_count (5)\n","        Number of topics: 10\n","        Sparsity factor: 0.01342\n","        ============================\n","    \n","\n","Ran 200 iterations in 10.524 seconds\n","\n","\n","Top topic terms\n","================\n","Topic 1: heston, giuliani, nra, hillary, lazio\n","Topic 2: wis, aflcio, teamster, ralph, nader\n","Topic 3: saving, elderly, trillion, surplus, medicare\n","Topic 4: conrad, donor, donation, gasoline, reno\n","Topic 5: vetting, edition, sarasota, mckinnon, coelho\n","Topic 6: dna, execute, inmate, penalty, death\n","Topic 7: nato, treaty, russia, troop, nuclear\n","Topic 8: danforth, scalia, plank, jew, naacp\n","Topic 9: hance, prince, castro, ranger, yale\n","Topic 10: oprah, karenna, schiff, kiss, vidal\n","Causal Topics:,  [ 2  4  5  7 10]\n","\n","\n","=> Iternation:  1\n","\n","\n","Running pLSA\n","=============\n","\n","        Running numba pLSA algorithm\n","        ============================\n","        Number of iterations: 200\n","        Number of documents: 3625 / 3625 before min_count (5)\n","        Number of terms: 6301 / 14916 before min_count (5)\n","        Number of topics: 10\n","        Sparsity factor: 0.01342\n","        ============================\n","    \n","\n","Ran 200 iterations in 10.836 seconds\n","\n","\n","Top topic terms\n","================\n","Topic 1: heston, giuliani, nra, hillary, lazio\n","Topic 2: wis, aflcio, teamster, ralph, nader\n","Topic 3: saving, elderly, trillion, surplus, medicare\n","Topic 4: conrad, donor, donation, gasoline, reno\n","Topic 5: vetting, edition, sarasota, mckinnon, coelho\n","Topic 6: dna, execute, inmate, penalty, death\n","Topic 7: nato, treaty, russia, troop, nuclear\n","Topic 8: scalia, plank, naacp, jew, abortion\n","Topic 9: hance, andover, prince, castro, yale\n","Topic 10: oprah, karenna, schiff, kiss, vidal\n","Causal Topics:,  [ 2  4  5  7 10]\n","\n","\n","=> Iternation:  2\n","\n","\n","Running pLSA\n","=============\n","\n","        Running numba pLSA algorithm\n","        ============================\n","        Number of iterations: 200\n","        Number of documents: 3625 / 3625 before min_count (5)\n","        Number of terms: 6301 / 14916 before min_count (5)\n","        Number of topics: 10\n","        Sparsity factor: 0.01342\n","        ============================\n","    \n","\n","Ran 200 iterations in 10.368 seconds\n","\n","\n","Top topic terms\n","================\n","Topic 1: heston, giuliani, nra, hillary, lazio\n","Topic 2: wis, aflcio, teamster, ralph, nader\n","Topic 3: saving, elderly, trillion, surplus, medicare\n","Topic 4: conrad, donor, donation, gasoline, reno\n","Topic 5: vetting, edition, sarasota, mckinnon, coelho\n","Topic 6: dna, execute, inmate, penalty, death\n","Topic 7: nato, treaty, russia, troop, nuclear\n","Topic 8: scalia, plank, naacp, jew, abortion\n","Topic 9: hance, andover, prince, castro, yale\n","Topic 10: oprah, karenna, schiff, kiss, vidal\n","Causal Topics:,  [ 2  4  5  7 10]\n","\n","\n","=> Iternation:  3\n","\n","\n","Running pLSA\n","=============\n","\n","        Running numba pLSA algorithm\n","        ============================\n","        Number of iterations: 200\n","        Number of documents: 3625 / 3625 before min_count (5)\n","        Number of terms: 6301 / 14916 before min_count (5)\n","        Number of topics: 10\n","        Sparsity factor: 0.01342\n","        ============================\n","    \n","\n","Ran 200 iterations in 10.389 seconds\n","\n","\n","Top topic terms\n","================\n","Topic 1: heston, giuliani, nra, hillary, lazio\n","Topic 2: wis, aflcio, teamster, ralph, nader\n","Topic 3: saving, elderly, trillion, surplus, medicare\n","Topic 4: conrad, donor, gasoline, donation, reno\n","Topic 5: edition, vetting, sarasota, mckinnon, coelho\n","Topic 6: dna, execute, inmate, penalty, death\n","Topic 7: nato, treaty, russia, troop, nuclear\n","Topic 8: scalia, plank, naacp, jew, abortion\n","Topic 9: hance, andover, prince, castro, yale\n","Topic 10: oprah, karenna, schiff, kiss, vidal\n","Causal Topics:,  [ 2  4  5  7 10]\n","\n","\n","=> Iternation:  4\n","\n","\n","Running pLSA\n","=============\n","\n","        Running numba pLSA algorithm\n","        ============================\n","        Number of iterations: 200\n","        Number of documents: 3625 / 3625 before min_count (5)\n","        Number of terms: 6301 / 14916 before min_count (5)\n","        Number of topics: 10\n","        Sparsity factor: 0.01342\n","        ============================\n","    \n","\n","Ran 200 iterations in 10.405 seconds\n","\n","\n","Top topic terms\n","================\n","Topic 1: heston, giuliani, nra, hillary, lazio\n","Topic 2: wis, aflcio, teamster, ralph, nader\n","Topic 3: saving, elderly, trillion, surplus, medicare\n","Topic 4: conrad, donor, gasoline, donation, reno\n","Topic 5: edition, vetting, sarasota, mckinnon, coelho\n","Topic 6: dna, execute, inmate, penalty, death\n","Topic 7: nato, treaty, russia, troop, nuclear\n","Topic 8: scalia, plank, naacp, jew, abortion\n","Topic 9: hance, andover, prince, castro, yale\n","Topic 10: oprah, karenna, schiff, kiss, vidal\n","Causal Topics:,  [ 2  4  5  7 10]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QOYDzcC0AHse"},"source":[""],"execution_count":null,"outputs":[]}]}